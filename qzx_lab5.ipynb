{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(max_depth=2)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "housing = fetch_california_housing(data_home = \"./dataset\")\n",
    "from sklearn import tree\n",
    "dtr = tree.DecisionTreeRegressor(max_depth = 2)\n",
    "dtr.fit(housing.data[:, [6, 7]], housing.target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8087175262588111"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "data_train, data_test, target_train, target_test = \\\n",
    "    train_test_split(housing.data, housing.target, test_size = 0.1, random_state = 42)\n",
    "dtr = tree.DecisionTreeRegressor(random_state = 42)\n",
    "dtr.fit(data_train, target_train)\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "rfr = RandomForestRegressor( random_state = 42)\n",
    "rfr.fit(data_train, target_train)\n",
    "rfr.score(data_test, target_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "({'mean_fit_time': array([0.94726624, 4.67629867, 9.69735117, 0.91801667, 4.70922403,\n",
       "         8.91783767, 0.86831312, 4.35891781, 8.77059855]),\n",
       "  'std_fit_time': array([0.01927267, 0.05965839, 0.46192059, 0.09083373, 0.21448334,\n",
       "         0.12489564, 0.01417273, 0.05974506, 0.36978989]),\n",
       "  'mean_score_time': array([0.0112865 , 0.05219355, 0.10556355, 0.01059265, 0.04607406,\n",
       "         0.08900175, 0.0089808 , 0.04169784, 0.08702106]),\n",
       "  'std_score_time': array([4.08488585e-04, 7.42755905e-04, 5.35153182e-03, 1.73176207e-03,\n",
       "         1.32171262e-03, 4.58900196e-03, 3.61734966e-05, 7.58724634e-04,\n",
       "         9.02361102e-03]),\n",
       "  'param_min_samples_split': masked_array(data=[3, 3, 3, 6, 6, 6, 9, 9, 9],\n",
       "               mask=[False, False, False, False, False, False, False, False,\n",
       "                     False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'param_n_estimators': masked_array(data=[10, 50, 100, 10, 50, 100, 10, 50, 100],\n",
       "               mask=[False, False, False, False, False, False, False, False,\n",
       "                     False],\n",
       "         fill_value='?',\n",
       "              dtype=object),\n",
       "  'params': [{'min_samples_split': 3, 'n_estimators': 10},\n",
       "   {'min_samples_split': 3, 'n_estimators': 50},\n",
       "   {'min_samples_split': 3, 'n_estimators': 100},\n",
       "   {'min_samples_split': 6, 'n_estimators': 10},\n",
       "   {'min_samples_split': 6, 'n_estimators': 50},\n",
       "   {'min_samples_split': 6, 'n_estimators': 100},\n",
       "   {'min_samples_split': 9, 'n_estimators': 10},\n",
       "   {'min_samples_split': 9, 'n_estimators': 50},\n",
       "   {'min_samples_split': 9, 'n_estimators': 100}],\n",
       "  'split0_test_score': array([0.78921037, 0.80784897, 0.81028215, 0.78546578, 0.80886544,\n",
       "         0.81182242, 0.79886058, 0.80689261, 0.80762266]),\n",
       "  'split1_test_score': array([0.77586603, 0.80166129, 0.80201073, 0.7868611 , 0.7969492 ,\n",
       "         0.80228209, 0.78450855, 0.80082571, 0.80091609]),\n",
       "  'split2_test_score': array([0.78468141, 0.80349054, 0.80431441, 0.79140981, 0.80148813,\n",
       "         0.80312577, 0.78609638, 0.79849707, 0.80292856]),\n",
       "  'split3_test_score': array([0.78926853, 0.80830274, 0.81058995, 0.79769623, 0.81014112,\n",
       "         0.81122596, 0.79127835, 0.80660557, 0.81211344]),\n",
       "  'split4_test_score': array([0.78834207, 0.80728844, 0.80714382, 0.79035073, 0.80574601,\n",
       "         0.80617445, 0.7926425 , 0.80518556, 0.80578927]),\n",
       "  'mean_test_score': array([0.78547368, 0.80571839, 0.80686821, 0.79035673, 0.80463798,\n",
       "         0.80692614, 0.79067727, 0.8036013 , 0.805874  ]),\n",
       "  'std_test_score': array([0.00508971, 0.00264978, 0.00333764, 0.00426891, 0.00486667,\n",
       "         0.00397585, 0.00510348, 0.00335038, 0.00387949]),\n",
       "  'rank_test_score': array([9, 4, 2, 8, 5, 1, 7, 6, 3])},\n",
       " {'min_samples_split': 6, 'n_estimators': 100},\n",
       " 0.806926138029568)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "tree_param_grid = { 'min_samples_split': list((3,6,9)),'n_estimators':list((10,50,100))}\n",
    "grid = GridSearchCV(RandomForestRegressor(),param_grid=tree_param_grid, cv=5)\n",
    "grid.fit(data_train, target_train)\n",
    "grid.cv_results_, grid.best_params_, grid.best_score_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MedInc        0.522452\n",
       "AveOccup      0.137967\n",
       "Latitude      0.090634\n",
       "Longitude     0.089608\n",
       "HouseAge      0.054167\n",
       "AveRooms      0.044934\n",
       "Population    0.030706\n",
       "AveBedrms     0.029530\n",
       "dtype: float64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(rfr.feature_importances_, index = housing.feature_names).sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   891.000000  891.000000  891.000000  714.000000  891.000000   \n",
      "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
      "std     257.353842    0.486592    0.836071   14.526497    1.102743   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     223.500000    0.000000    2.000000   20.125000    0.000000   \n",
      "50%     446.000000    0.000000    3.000000   28.000000    0.000000   \n",
      "75%     668.500000    1.000000    3.000000   38.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
      "\n",
      "            Parch        Fare  \n",
      "count  891.000000  891.000000  \n",
      "mean     0.381594   32.204208  \n",
      "std      0.806057   49.693429  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    7.910400  \n",
      "50%      0.000000   14.454200  \n",
      "75%      0.000000   31.000000  \n",
      "max      6.000000  512.329200  \n",
      "       PassengerId    Survived      Pclass         Age       SibSp  \\\n",
      "count   891.000000  891.000000  891.000000  891.000000  891.000000   \n",
      "mean    446.000000    0.383838    2.308642   29.699118    0.523008   \n",
      "std     257.353842    0.486592    0.836071   13.002015    1.102743   \n",
      "min       1.000000    0.000000    1.000000    0.420000    0.000000   \n",
      "25%     223.500000    0.000000    2.000000   22.000000    0.000000   \n",
      "50%     446.000000    0.000000    3.000000   29.699118    0.000000   \n",
      "75%     668.500000    1.000000    3.000000   35.000000    1.000000   \n",
      "max     891.000000    1.000000    3.000000   80.000000    8.000000   \n",
      "\n",
      "            Parch        Fare  \n",
      "count  891.000000  891.000000  \n",
      "mean     0.381594   32.204208  \n",
      "std      0.806057   49.693429  \n",
      "min      0.000000    0.000000  \n",
      "25%      0.000000    7.910400  \n",
      "50%      0.000000   14.454200  \n",
      "75%      0.000000   31.000000  \n",
      "max      6.000000  512.329200  \n"
     ]
    }
   ],
   "source": [
    "import pandas \n",
    "titanic = pandas.read_csv(\"titanic_train.csv\")\n",
    "titanic.head(5)s\n",
    "print (titanic.describe())\n",
    "titanic[\"Age\"] = titanic[\"Age\"].fillna(titanic[\"Age\"].mean())\n",
    "print (titanic.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "titanic.loc[titanic[\"Sex\"] == \"male\", \"Sex\"] = 0\n",
    "titanic.loc[titanic[\"Sex\"] == \"female\", \"Sex\"] = 1\n",
    "titanic[\"Embarked\"] = titanic[\"Embarked\"].fillna('S')\n",
    "titanic.loc[titanic[\"Embarked\"] == \"S\", \"Embarked\"] = 0\n",
    "titanic.loc[titanic[\"Embarked\"] == \"C\", \"Embarked\"] = 1\n",
    "titanic.loc[titanic[\"Embarked\"] == \"Q\", \"Embarked\"] = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7946142740568704\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\33131\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n",
      "C:\\Users\\33131\\AppData\\Roaming\\Python\\Python37\\site-packages\\sklearn\\linear_model\\_logistic.py:818: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG,\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "alg = LogisticRegression(random_state=1)\n",
    "scores = cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=5)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8047517418868871\n",
      "0.8215805661917017\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "predictors = [\"Pclass\", \"Sex\", \"Age\", \"SibSp\", \"Parch\", \"Fare\", \"Embarked\"]\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=10, min_samples_split=2, min_samples_leaf=1)\n",
    "scores = cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=5)\n",
    "print(scores.mean())\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=50, min_samples_split=4, min_samples_leaf=2)\n",
    "scores = cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=5)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mr          517\n",
      "Miss        182\n",
      "Mrs         125\n",
      "Master       40\n",
      "Dr            7\n",
      "Rev           6\n",
      "Major         2\n",
      "Col           2\n",
      "Mlle          2\n",
      "Ms            1\n",
      "Jonkheer      1\n",
      "Mme           1\n",
      "Sir           1\n",
      "Lady          1\n",
      "Countess      1\n",
      "Don           1\n",
      "Capt          1\n",
      "Name: Name, dtype: int64\n",
      "1     517\n",
      "2     183\n",
      "3     125\n",
      "4      40\n",
      "5       7\n",
      "6       6\n",
      "7       5\n",
      "10      3\n",
      "8       3\n",
      "9       2\n",
      "Name: Name, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "titanic[\"FamilySize\"] = titanic[\"SibSp\"] + titanic[\"Parch\"]\n",
    "titanic[\"NameLength\"] = titanic[\"Name\"].apply(lambda x: len(x))\n",
    "import re\n",
    "def get_title(name):\n",
    "    title_search = re.search(' ([A-Za-z]+)\\.', name)\n",
    "    if title_search:\n",
    "        return title_search.group(1)\n",
    "    return \"\"\n",
    "titles = titanic[\"Name\"].apply(get_title)\n",
    "print(pandas.value_counts(titles))\n",
    "title_mapping = {\"Mr\": 1, \"Miss\": 2, \"Mrs\": 3, \"Master\": 4, \"Dr\": 5, \"Rev\": 6, \"Major\": 7, \"Col\": 7, \"Mlle\": 8, \"Mme\": 8, \"Don\": 9, \"Lady\": 10, \"Countess\": 10, \"Jonkheer\": 10, \"Sir\": 9, \"Capt\": 7, \"Ms\": 2}\n",
    "for k,v in title_mapping.items():\n",
    "    titles[titles == k] = v\n",
    "print(pandas.value_counts(titles))\n",
    "titanic[\"Title\"] = titles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8182035026049839\n",
      "0.8350448810495262\n"
     ]
    }
   ],
   "source": [
    "predictors = [\"Pclass\", \"Sex\", \"Fare\", \"Title\"]\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=50, min_samples_split=8, min_samples_leaf=4)\n",
    "scores = cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=5)\n",
    "print(scores.mean())\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "predictors = [\"Pclass\", \"Sex\", \"Fare\", \"Embarked\",\"Title\",\"NameLength\"]\n",
    "alg = RandomForestClassifier(random_state=1, n_estimators=100, min_samples_split=4, min_samples_leaf=2)\n",
    "scores = cross_val_score(alg, titanic[predictors], titanic[\"Survived\"], cv=5)\n",
    "print(scores.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
